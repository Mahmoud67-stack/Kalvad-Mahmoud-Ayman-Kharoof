NYC Yellow Taxi Data Analysis and Pipeline Project
Project Overview
This project involves working with NYC Yellow Taxi data for the month of January 2024. The project is divided into several parts, including SQL challenges, data pipeline design, data processing with Python, and data visualization. The goal is to automate data ingestion, transformation, loading, and analysis using Python and PostgreSQL, with Airflow as the orchestrator for managing the pipeline.

Dataset
The dataset used is the NYC Yellow Taxi Trip Records, specifically for January 2024. It contains fields such as pickup and drop-off times, trip distances, fare amounts, and payment types. The data is publicly available and was obtained in .parquet format.

Project Structure
Part 1: SQL Challenges
Part 2: Data Pipeline Design
Part 3: Data Processing with Python
Part 4: Data Visualization
Each part addresses specific data challenges, from querying and transforming data to visualizing insights.

Table of Contents
Project Overview
Dataset
Project Structure
Installation
Pipeline Overview
Data Insights
Data Visualization
Results
Contributors
Installation
Requirements
To run this project, you’ll need the following:

Python (Version 3.8 or later)
PostgreSQL (with TimescaleDB if available)
Apache Airflow
Python Libraries: See requirements.txt for a full list of dependencies.
Setting Up the Environment
Clone the Repository:

bash
Copy code
git clone <your-repo-url>
cd <repository-folder>
Install Dependencies:

bash
Copy code
pip install -r requirements.txt
Configure PostgreSQL:

Ensure PostgreSQL is running.
Create a database (e.g., nyc_taxi_data).
Update the PostgreSQL connection details in the Python script as per your setup.
Airflow Setup:

Set up Airflow and start the web server to manage the pipeline.
Pipeline Overview
Steps in the Pipeline
Download Data:

Checks if the .parquet file for the specified month exists locally. If not, it downloads it from a public URL and saves it locally.
Transform Data:

Loads the .parquet file, filters trips with fares above $10, renames columns, and performs quality checks to remove rows with null values or invalid data.
Load Data into PostgreSQL:

Loads the transformed data into PostgreSQL in batches to handle large data files efficiently.
Calculate Average Fare by Day of the Week:

Analyzes the data to calculate the average fare per day for each day of the week in January 2024, storing the results in a CSV file.
Data Visualization:

Creates a time series chart showing the total revenue per day for January 2024.
Airflow Configuration
Retry Settings: Each task is configured to retry up to three times in case of failure, with a 5-minute delay between retries.
Email Notifications: Configured to send notifications on task failure, making it easier to monitor the pipeline.
Data Insights
SQL Analysis
Total Revenue Calculation:

Calculated total revenue generated by trips that ended within the last 30 days of the data.
Top 3 Pickup Locations:

Found the top 3 pickup locations by total revenue in January 2024.
Frequent Riders:

Identified locations with passengers completing more than 5 trips within the last 30 days of data (if available).
Average Fare Analysis (Python Script)
Highest Average Fare: Monday with an average fare of $19.36.
Lowest Average Fare: Saturday with an average fare of $16.97.
Overall Trend: Higher fares on weekdays, potentially indicating increased demand at the start of the workweek.
Data Visualization
The visualization focuses on Total Revenue Per Day for January 2024, highlighting fluctuations, peaks, and dips. Here’s a summary:

Fluctuations: Daily revenue fluctuates between $1.25 million and $2 million.
Peaks: Highest revenue days occur around January 3rd, 13th, 17th, and 25th, potentially aligning with weekends or events.
End-of-Month Stability: The revenue trend stabilizes toward the end of January.


Results
This project successfully:

Automated data ingestion, transformation, and loading using Airflow.
Enabled detailed SQL analysis to extract revenue-related insights.
Visualized revenue trends, providing insights into demand patterns.
The project showcases the benefits of using a well-orchestrated data pipeline to manage, analyze, and visualize large datasets.

Contributors
Mahmoud Ayman Kharoof